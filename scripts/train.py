#!/usr/bin/env python3
"""
DevMentor AI - Main Training Script

Orchestrates the complete training pipeline:
1. Pre-training
2. Fine-tuning
3. Constitutional alignment
"""

import argparse
import logging
import sys
from pathlib import Path
import yaml
import torch

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_config(config_path: str) -> dict:
    """Load training configuration from YAML file"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def setup_distributed():
    """Setup distributed training environment"""
    if torch.cuda.is_available():
        logger.info(f"CUDA available: {torch.cuda.device_count()} GPUs")
        for i in range(torch.cuda.device_count()):
            logger.info(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        logger.warning("CUDA not available, using CPU")


def pretrain(config: dict):
    """
    Pre-training stage

    Args:
        config: Training configuration
    """
    logger.info("=" * 60)
    logger.info("STAGE 1: PRE-TRAINING")
    logger.info("=" * 60)

    from src.training.pretraining.model import create_model

    # Create model
    model_size = config['model']['size']
    logger.info(f"Creating {model_size} model...")
    model = create_model(model_size)

    num_params = sum(p.numel() for p in model.parameters())
    logger.info(f"Model parameters: {num_params / 1e6:.1f}M")

    # Load data
    logger.info("Loading training data...")
    # data_loader = create_data_loader(config['pretraining'])

    # Training loop
    logger.info("Starting pre-training...")
    # train_loop(model, data_loader, config['pretraining'])

    # Save checkpoint
    output_dir = Path(config['pretraining']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)

    checkpoint_path = output_dir / "pretrained_model.pt"
    # torch.save(model.state_dict(), checkpoint_path)
    logger.info(f"Pre-training complete. Model saved to {checkpoint_path}")

    return model


def finetune(config: dict, pretrained_model=None):
    """
    Fine-tuning stage

    Args:
        config: Training configuration
        pretrained_model: Pre-trained model (optional)
    """
    logger.info("=" * 60)
    logger.info("STAGE 2: FINE-TUNING")
    logger.info("=" * 60)

    # Load or use provided model
    if pretrained_model is None:
        from src.training.pretraining.model import create_model
        model = create_model(config['model']['size'])
        # Load pre-trained weights
        # model.load_state_dict(torch.load("checkpoints/pretraining/pretrained_model.pt"))
    else:
        model = pretrained_model

    # Fine-tuning tasks
    tasks = config['finetuning']['tasks']
    logger.info(f"Fine-tuning on tasks: {tasks}")

    # Training loop
    logger.info("Starting fine-tuning...")
    # finetune_loop(model, data_loader, config['finetuning'])

    # Save checkpoint
    output_dir = Path(config['finetuning']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)

    checkpoint_path = output_dir / "finetuned_model.pt"
    # torch.save(model.state_dict(), checkpoint_path)
    logger.info(f"Fine-tuning complete. Model saved to {checkpoint_path}")

    return model


def align(config: dict, finetuned_model=None):
    """
    Constitutional alignment stage

    Args:
        config: Training configuration
        finetuned_model: Fine-tuned model (optional)
    """
    logger.info("=" * 60)
    logger.info("STAGE 3: CONSTITUTIONAL ALIGNMENT")
    logger.info("=" * 60)

    from src.training.alignment.constitutional_ai import Constitution, ConstitutionalAITrainer

    # Load model
    if finetuned_model is None:
        from src.training.pretraining.model import create_model
        model = create_model(config['model']['size'])
    else:
        model = finetuned_model

    # Load constitution
    constitution = Constitution()
    logger.info(f"Loaded {len(constitution.principles)} constitutional principles")

    # Constitutional AI training
    if config['alignment']['constitutional']['enabled']:
        logger.info("Applying Constitutional AI...")
        # trainer = ConstitutionalAITrainer(model, tokenizer, constitution)
        # trainer.generate_training_dataset(prompts)

    # RLHF training
    if config['alignment']['rlhf']['enabled']:
        logger.info("Starting RLHF training...")
        # rlhf_train(model, config['alignment']['rlhf'])

    # Save aligned model
    output_dir = Path(config['alignment']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)

    checkpoint_path = output_dir / "aligned_model.pt"
    # torch.save(model.state_dict(), checkpoint_path)
    logger.info(f"Alignment complete. Model saved to {checkpoint_path}")

    return model


def evaluate_model(model, config: dict):
    """
    Evaluate model on benchmarks

    Args:
        model: Model to evaluate
        config: Evaluation configuration
    """
    logger.info("=" * 60)
    logger.info("MODEL EVALUATION")
    logger.info("=" * 60)

    from src.evaluation.benchmarks.code_evaluation import HumanEvalBenchmark

    benchmarks = config['evaluation']['benchmarks']
    logger.info(f"Running benchmarks: {benchmarks}")

    results = {}

    # HumanEval
    if 'humaneval' in benchmarks:
        logger.info("Running HumanEval benchmark...")
        benchmark = HumanEvalBenchmark()
        # results['humaneval'] = benchmark.evaluate_model(model, tokenizer)

    logger.info("Evaluation complete")
    logger.info(f"Results: {results}")

    return results


def main():
    """Main training pipeline"""
    parser = argparse.ArgumentParser(description="DevMentor AI Training Pipeline")
    parser.add_argument(
        "--config",
        type=str,
        default="config/training_config.yaml",
        help="Path to training configuration file"
    )
    parser.add_argument(
        "--stage",
        type=str,
        choices=["pretraining", "finetuning", "alignment", "all"],
        default="all",
        help="Training stage to run"
    )
    parser.add_argument(
        "--eval",
        action="store_true",
        help="Run evaluation after training"
    )
    parser.add_argument(
        "--resume",
        type=str,
        help="Path to checkpoint to resume from"
    )

    args = parser.parse_args()

    # Load configuration
    logger.info(f"Loading configuration from {args.config}")
    config = load_config(args.config)

    # Setup distributed training
    setup_distributed()

    # Set random seed for reproducibility
    if config.get('seed'):
        torch.manual_seed(config['seed'])
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(config['seed'])

    # Run training stages
    model = None

    try:
        if args.stage in ["pretraining", "all"]:
            model = pretrain(config)

        if args.stage in ["finetuning", "all"]:
            model = finetune(config, model)

        if args.stage in ["alignment", "all"]:
            model = align(config, model)

        # Evaluation
        if args.eval and model is not None:
            evaluate_model(model, config)

        logger.info("=" * 60)
        logger.info("âœ… TRAINING PIPELINE COMPLETE")
        logger.info("=" * 60)

    except KeyboardInterrupt:
        logger.info("Training interrupted by user")
    except Exception as e:
        logger.error(f"Training failed: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
