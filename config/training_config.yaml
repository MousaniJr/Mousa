# DevMentor AI Training Configuration

# Model Configuration
model:
  size: "medium"  # small, medium, large, xl
  vocab_size: 50000
  hidden_size: 2048
  num_layers: 24
  num_heads: 16
  max_seq_len: 4096
  dropout: 0.1
  use_flash_attention: true

# Pre-training Configuration
pretraining:
  # Data
  data_dir: "data/processed"
  train_split: 0.95
  val_split: 0.05

  # Training
  batch_size: 512  # Global batch size
  micro_batch_size: 4  # Per-device batch size
  gradient_accumulation_steps: 128
  num_epochs: 1
  max_steps: 500000

  # Optimization
  learning_rate: 3.0e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 1.0e-8
  warmup_steps: 2000
  lr_scheduler: "cosine"  # linear, cosine, polynomial
  gradient_clipping: 1.0

  # Precision
  mixed_precision: "bf16"  # fp16, bf16, fp32

  # Checkpointing
  save_steps: 1000
  eval_steps: 500
  logging_steps: 10
  save_total_limit: 5
  output_dir: "checkpoints/pretraining"

  # Distributed Training
  distributed:
    backend: "nccl"
    zero_stage: 3  # DeepSpeed ZeRO stage
    offload_optimizer: true
    offload_param: false
    gradient_checkpointing: true

# Fine-tuning Configuration
finetuning:
  # Data
  data_dir: "data/finetuning"
  train_file: "train.jsonl"
  val_file: "val.jsonl"

  # Training
  batch_size: 128
  micro_batch_size: 4
  gradient_accumulation_steps: 32
  num_epochs: 3
  max_steps: 10000

  # Optimization
  learning_rate: 1.0e-5  # Lower than pre-training
  weight_decay: 0.01
  warmup_steps: 100
  gradient_clipping: 1.0

  # Checkpointing
  save_steps: 500
  eval_steps: 250
  output_dir: "checkpoints/finetuning"

  # Task Types
  tasks:
    - code_completion
    - bug_fixing
    - code_review
    - refactoring
    - documentation

# Alignment Configuration
alignment:
  # Constitutional AI
  constitutional:
    enabled: true
    num_iterations: 2
    principles_file: "docs/CONSTITUTION.md"

  # RLHF
  rlhf:
    enabled: true

    # Reward Model Training
    reward_model:
      data_file: "data/comparisons.jsonl"
      batch_size: 32
      learning_rate: 1.0e-5
      num_epochs: 3
      hidden_size: 1024
      num_layers: 12

    # PPO Training
    ppo:
      learning_rate: 1.0e-6
      kl_penalty: 0.1
      clip_ratio: 0.2
      value_clip: 0.2
      num_epochs: 1
      batch_size: 64
      minibatch_size: 16

  # Checkpointing
  save_steps: 500
  output_dir: "checkpoints/alignment"

# Continuous Learning Configuration
continuous_learning:
  enabled: true
  feedback_dir: "data/feedback"
  min_feedback_for_retrain: 1000

  # Retraining Schedule
  schedule:
    weekly_finetuning:
      enabled: true
      day: "Sunday"
      time: "02:00"
      min_examples: 1000

    monthly_retrain:
      enabled: true
      day: 1
      time: "02:00"
      min_examples: 10000

# Evaluation Configuration
evaluation:
  benchmarks:
    - humaneval
    - mbpp
    - dev_tasks

  humaneval:
    max_problems: 164
    timeout: 5

  mbpp:
    max_problems: 500

  dev_tasks:
    custom_tasks: true

  # Evaluation Schedule
  eval_during_training: true
  eval_steps: 1000
  eval_on_startup: true

# Hardware Configuration
hardware:
  # GPU
  num_gpus: 8
  gpu_type: "A100"  # V100, A100, H100
  gpu_memory: "80GB"

  # CPU
  num_cpus: 64
  cpu_memory: "512GB"

  # Storage
  storage_type: "NVMe SSD"
  storage_capacity: "10TB"

# Logging and Monitoring
logging:
  # Experiment Tracking
  use_wandb: true
  wandb_project: "devmentor-ai"
  wandb_entity: "your-username"

  use_tensorboard: true
  tensorboard_dir: "runs"

  use_mlflow: false

  # Logging Levels
  log_level: "INFO"
  log_file: "logs/training.log"

  # Metrics
  log_metrics: true
  log_gradients: false
  log_model_stats: true

# Safety and Compliance
safety:
  # Security Validation
  validate_training_data: true
  scan_for_secrets: true
  filter_pii: true

  # Content Filtering
  filter_offensive_content: true
  filter_copyrighted_code: true

  # Monitoring
  detect_output_anomalies: true
  flag_unsafe_generations: true

# Reproducibility
seed: 42
deterministic: true
